{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43df654b-3167-4698-8922-62b7cf4aaa99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Volumes/streeming/demo_db/datasource'\n",
    "\n",
    "spark.sql(\"drop table if exists streeming.demo_db.word_count\")\n",
    "\n",
    "dbutils.fs.rm(f\"{base_dir}/checkpointing\",True)\n",
    "\n",
    "dbutils.fs.rm(f\"{base_dir}/data/text\",True)\n",
    "\n",
    "dbutils.fs.mkdirs(f\"{base_dir}/checkpointing\")\n",
    "\n",
    "dbutils.fs.mkdirs(f\"{base_dir}/data/text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fc037b-b710-417e-ae97-1a52d3ef5849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BatchWC():\n",
    "    def __init__(self,base_dir):\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        from pyspark.sql.functions import split, col, explode\n",
    "        lines_df = spark.read.format(\"text\").option(\"lineSep\",\".\").load(f\"{self.base_dir}/data/text\")\n",
    "        raw_words_df = lines_df.select(\"value\")\\\n",
    "                        .withColumn(\"word\",explode(split(col(\"value\"),\" \")))\\\n",
    "                        .drop(\"value\")\n",
    "        return raw_words_df\n",
    "    \n",
    "    def get_qality_word_count(self,df):\n",
    "        from pyspark.sql.functions import lower,trim,col\n",
    "        quality_word_df = df.withColumn(\"word\",lower(trim(col(\"word\"))))\\\n",
    "                              .filter(col(\"word\").isNotNull() & col(\"word\").rlike('[a-z]'))\n",
    "        return quality_word_df\n",
    "    \n",
    "    def get_result(self,df):\n",
    "        from pyspark.sql.functions import col\n",
    "        result_df = df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "        return result_df\n",
    "        \n",
    "\n",
    "    def write_result(self,df):\n",
    "        df.write.mode(\"append\").format(\"delta\").saveAsTable(\"streeming.demo_db.word_count\")\n",
    "\n",
    "\n",
    "    def word_count(self):\n",
    "        raw_words_df = self.get_raw_data()\n",
    "        quality_word_df = self.get_qality_word_count(raw_words_df)\n",
    "        result_df = self.get_result(quality_word_df)\n",
    "        self.write_result(result_df)\n",
    "        print(\"Done Writing Result to DB\")\n",
    "\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6059400935122651,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "streeming_word_count",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
